{"title":"AWS Deepracer RL競賽實戰心得分享","date":"2019-06-12T16:46:25.000Z","date_formatted":{"ll":"Jun 13, 2019","L":"06/13/2019","MM-DD":"06-13"},"thumbnail":"https://cdn-images-1.medium.com/max/2000/1*F63CNm2iTQUbgBGzLt-1Yg.png","link":"2019/06/13/AWS_DeepRacer","comments":true,"tags":["Amazon Web Services","Deep Reinforcement Learning","Deepracer"],"categories":["Reinforcement Learning"],"updated":"2020-04-13T23:55:01.571Z","content":"<p>近期內AWS推廣Deepracer挑戰賽特別火熱，究竟是什麼東西讓如此著迷呢，接下來細細聽我分享。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*F63CNm2iTQUbgBGzLt-1Yg.png\" class=\"φcy\" alt=\"AWS Deepracer League\"><em>AWS Deepracer League</em></p>\n<p>最近非常火紅的屬自動駕駛、機器學習了，但這些東西對於一般人來說實在是有距離感，也無從入門，所幸Amazon在這方面替大家做了很多工，跟Intel NervanaSystems的<a href=\"https://nervanasystems.github.io/coach/\" target=\"_blank\">Reinforcement Learning Coach</a>專案合作，替大家免去了演算法及相關特徵工程的繁雜，整合使用ROS系統方便更快部署模型到實體車輛上，也提供了Gazebo訓練環境供車輛訓練，這些在增強式學習領域都是工程師的重要工程，AWS替大家把服務整合了在一起，提供了一個Console面板，後面的繁重工作都幫你處理好了。</p>\n<p>今天你只要訓練好模型，然後下載模型，放到車子上，就可以完成一台小車自動駕駛的夢想（想想看以前的爆走兄弟卡通，他會自己跑是多麽浪漫的事，只差不會噴火了），話不多說，直接來介紹今天主角：AWS Deepracer Car</p>\n<p>AWS Deepracer Car本身是一個1/18的實體車輛，他的優點在於本身是一台類似Donkey Car ，但是運算能力超群，車輛規格如下（取自<a href=\"https://aws.amazon.com/tw/deepracer/faqs/\" target=\"_blank\">AWS</a>）：</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*kW49cwafVp4FKiFy7woGXg.png\" class=\"φcy\"></p>\n<ol><li><p>汽車：1/18 比例 4 輪驅動並加裝引擎越野卡車底盤</p>\n</li>\n<li><p>CPU：Intel Atom™ 處理器</p>\n</li>\n<li><p>記憶體：4 GB RAM</p>\n</li>\n<li><p>儲存體：32 GB (可擴充)</p>\n</li>\n<li><p>Wi-Fi：802.11ac</p>\n</li>\n<li><p>攝影機：採用 MJPEG 的 4MP 攝影機</p>\n</li>\n<li><p>軟體：Ubuntu OS 16.04.3 LTS、Intel® OpenVINO™ 工具套件、ROS Kinetic</p>\n</li>\n<li><p>驅動電池：7.4V/1100mAh 鋰聚合物</p>\n</li>\n<li><p>運算電池：13600mAh USB-C PD</p>\n</li>\n<li><p>連接埠：4x USB-A、1x USB-C、1x Micro-USB、1x HDMI</p>\n</li>\n<li><p>感應器：整合式加速度計和陀螺儀</p>\n</li>\n</ol><p>一台車該有的什麼都有了！就差一顆炙熱的心（還要充飽電才行）。</p>\n<p>關於一些體驗營等等前置教學文章在LINE工程師在參加完體驗營的時候有寫了一篇很詳細很棒的文章（連結：<a href=\"https://engineering.linecorp.com/zh-hant/blog/aws-deepracer-2019/\" target=\"_blank\">AWS DeepRacer 自動駕駛賽車模擬實驗營心得分享</a>），這邊主要講一些調整參數及比賽的心路歷程：</p>\n<p>實際上AWS舉辦的這個競賽分為實體跟虛擬兩類別，都是在AWS線上訓練，之後訓練好的模型可以下載下來部署到實體車輛上面（目前都尚未發售，除了在比賽中獲得車輛以外基本上都是要預購），而在實體競賽中是以下載到USB，透過插入USB，並使用iPad連結到車子的Dashboard方式控制車子。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*wkrEzKLNBQOPYTOwyg6W7Q.png\" class=\"φcy\"></p>\n<p>在實體比賽的時候也是讓你拿著iPad控制最大速度來操作；<br>虛擬競賽的時候則是直接提交訓練好的模型，由評估系統來試跑車輛。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2380/1*nqu-2Q7F7jWh1436yBfrsQ.png\" class=\"φcy\" alt=\"官方獎勵\"><em>官方獎勵</em></p>\n<p>而最大獎勵就是去拉斯維加斯機加酒全包的集結各地冠軍來比賽，算是AWS一年一度的最大盛事，但本人胸無大志，只想要男人的浪漫小車（前十名皆有一台小車當獎勵或同值獎品，一切以<a href=\"https://d1.awsstatic.com/DeepRacer/AWS-DeepRacer-League-2019-Official-Rules-English-March-2019.pdf\" target=\"_blank\">官方說明</a>為主）。在參加完5月份的Deepracer體驗營之後，驚艷了一下，因為除了在專業課程以外，從來沒有在外面場合聽到Reinforcement Learning這麼完整的解說跟教學，獎勵等等的影響因素都在Keynote中舉重若輕的表明了，事實證明AWS很重視他們對於RL領域的推廣及賽事，也證明了他們在AI的軟實力。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/3438/1*UBOs0oaCABXWp1-j0nQDgA.jpeg\" class=\"φcy\"></p>\n<p>在結束體驗營的時候，跑去問了又高又帥講師跟Amanda相關內容，他們提到有拉斯維加斯的時候感覺就還好，但是提到前十名都有一台小車的時候，心裡就開始蠢蠢欲動了（因為自己有一台小鴨車，根本提升好幾個等級）。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/3104/1*bJPZEDEIzt2JANp0gtXa-Q.jpeg\" class=\"φcy\"></p>\n<p>回到家之後立刻開啟了上課的內容，研究起可以更改的超參數跟最近比賽的資訊，看到London Loop虛擬競賽剩下兩週，心想，不然就訓練看看好了，結果第一次訓練8小時提交之後大概13秒左右，結果提交的時候需要我填入名字，我原本想說寫個CKSun，後來想想，覺得全世界都不太認識台灣，然後台灣很多軟實力超強的人都沒被發現（除了大舉來台幾個科技巨頭以外），想說推廣一下台灣跟台北，就打個Taiwan-Taipei-CKSun，</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*6fxbx_Rb9LkLUHLCHV79RQ.jpeg\" class=\"φcy\"></p>\n<p>就這麼進入了前50名？？？？</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/4348/1*di7_sUUooszzeR-Nj_M4xQ.png\" class=\"φcy\"></p>\n<p>當然50名不是目標，為了讓我寫的台灣台北這幾個字可以讓大家看到<a href=\"https://aws.amazon.com/tw/deepracer/schedule-and-standings/leaderboard-virtual-london-loop-2019/\" target=\"_blank\">（全世界即時的排行榜）</a>，接下來就是增強式學習的基本幾個心法，我打開了ROBOMAKER，研究了地圖，發現有些彎是可以不用刻意去彎，開了基本的地圖先觀察Agent的實際行為，他會刻意的去找中線，也是因為我一開始用預設的獎勵函數所致，所以我特意把獎勵函數設定成：</p>\n<pre><code>def reward_function(params):\n    &apos;&apos;&apos;\n    Example of rewarding the agent to follow center line\n    &apos;&apos;&apos;\n\n    # Read input parameters\n    track_width = params[&apos;track_width&apos;]\n    distance_from_center = params[&apos;distance_from_center&apos;]\n    all_wheels_on_track = params[&apos;all_wheels_on_track&apos;]\n\n    if all_wheels_on_track and distance_from_center &lt;= (0.3 * track_width):\n        reward = 1\n    else:\n        reward = 1e-3  # likely crashed/ close to off track    \n\n    return float(reward)</code></pre><p>其實就是AWS預設的第三個獎勵函數，而訓練出來的結果就是看起來很不在意中線，但是會很在意出軌，我在這邊常常跟<a href=\"https://join.slack.com/t/deepracer-london/shared_invite/enQtNjMzMTgyNTExODk1LTQ0OGQ1OWI3ZTdlMzY3MjRkYmYxZjU3NTNlODU0MGNlYmUxMTg5NmJlNTI1NzEyYzE0ODMwOTU5NzYyM2UxZmI\" target=\"_blank\">Deepracer社群</a>的人說，獎勵越簡單越好，越是刻意的去寫複雜的獎勵就有點像是過度教育一個寵物，越是故意去引導，反而會讓他陷入一個你認為好的方向的迷思，有時候Agent他會自己找到最好的答案，要保留最多的空間讓他探索，限制掉那些規則之外的東西就好了。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2780/1*3r8mKoca6kG_sur9_ukdiQ.jpeg\" alt=\"[圖片出處](https://techcrunch.com/2017/03/24/steve-mnuchin-axios-ai-workforce/)\">)<em><a href=\"https://techcrunch.com/2017/03/24/steve-mnuchin-axios-ai-workforce/\" title=\"|block\" target=\"_blank\">圖片出處</a></em></p>\n<p>這個想法說真的見仁見智，我所謂簡單的意思不是Reward Function越少行越好，而是訂定獎勵的時候要大方向，剔除掉不必要的細節，在適當的地方放上懲罰，而不是為懲罰而罰。正所謂 Less is more, simple is power.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*0OKQIYKyW8ZfMuTNdNuePw.png\" class=\"φcy\"></p>\n<p>調整了獎勵函數之後，接下來就要思考動作空間的問題，Deepracer的動作空間選擇很少，就是幾個排列組合而已，動作空間越大，探索的範圍代表越多，也就代表Agent要在這些動作空間中找出規律的機會越少，總而言之，訓練時間就要拉很長，而動作空間大的好處是：操控越細微、可以在微小的弧度中修正角度等，但是壞處倒是比較多，訓練時間極長，而且操控越細微代表容錯率低（敏感）等，所以如何在動作空間大小中做取捨，還是要在腦海裡用這些模擬出一個完美的賽車應該有什麼樣的跑線（不是頭文字Ｄ，這裡沒有水溝蓋）。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/8542/1*dRjOhP9EVRl5dVZQIO3LNA.jpeg\" class=\"φcy\"></p>\n<p>而我們必須在這些完美跑線跟動作空間做出一個好的平衡，上面的表格有細微的修正（10度）有大幅度的修正（30度），還有從中取捨的修正（小轉彎），基本上已經滿足轉彎需求，而<strong>速度</strong>這方面需要多方面測試，速度跟每秒判斷次數極度相關。因為每次判斷完一次車子就會做一個動作直到下一次判斷，也就是說，如果每秒判斷10次，而一秒跑5公尺的話，等於一秒就跑了50公分，那代表著，這次他要轉30度，他至少要跑50公分的距離才會做出下一個動作，那就不只是30度那麼小的角度了。</p>\n<p>綜合上述思考過後，我選擇了直接最高速度去訓練，經過訓練了2個小時之後，有開始收斂的現象，就開始用調整參數的方式處理訓練步驟，這邊非常建議新手從做表格開始，每次訓練都做一個超參數的表格，剛開始的幾個小時都是從0.0005 Learning Rate開始，看獎勵函數慢慢降低，但不要太低，因為Reinforcement Learning最怕Overfit，適度泛化模型是最好的選擇（選其他地圖適當訓練等等），也很高興的模型開始學習如何去找出最佳路徑來過彎道，接下來我就提交了訓練了2+2+8個小時的模型上去，</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/4096/1*zZHIg4RuX9_lue40Ke0Khg.jpeg\" class=\"φcy\"></p>\n<p>結果一舉沖到了第五名，突然衝到世界排行榜上的感覺真的很棒，因為前面就說想把台灣掛上去的夢想突然被達成，接下來我就想要再接再厲，於是我再度微調Reward Function的距離從0.3調到0.4，拿掉了ALL on Track（全部輪子不一定要在賽道上），接下來也是2+2小時（記得調高Learning Rate)的一直觀察車子行為，然後再度提交，</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/4096/1*mXzGib-ZYsHvpEBbkX7mcA.jpeg\" class=\"φcy\"></p>\n<p>真的有效的感覺，但是穩定度變差了，不一定可以跑完整圈的次數變多了（因為賽道判定較為嚴格，只要兩個輪子在白線外，車子中心稍微外偏就算未完成），但在高速與穩定者兩方面必須做出取捨，於是調低學習率繼續訓練讓他習慣這個步驟，直接丟出8小時的長時間 Batch_size大（穩定更新）的訓練，也順便去參加Computex展，然後參觀期間就順手遠端上傳一個模型，結果還真的衝進第一領先，</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*6CId1i-J70Im0XrZNkhu0g.jpeg\" class=\"φcy\"></p>\n<p>頓時嚇到，因為夢想好像都達到了，但是比賽也未結束，我還想看看模型極限在哪邊，於是繼續縮進學習率訓練，隔天不知道是不是被我氣到還是什麼（他們搞不好都還在穩定訓練），突然一堆強勁的對手衝過我，不過我也不甘示弱的再丟出新的訓練成果，結果也就進步0.5秒。比賽尾端果真進入白熱化階段，壓力真的不是普通大，很怕被刷出排行榜。</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2980/1*CqiKvC9EAmIkGhwGBe0ydw.jpeg\" class=\"φcy\"></p>\n<p>而然，比賽剩下最後兩天半，結果遭遇了ROS公用庫被攻擊的問題，整個ＡAWS Deepracer的訓練任務跟評估任務全面停擺兩天，突然覺得是被陰了還是屎到不行…，後來恢復正常的時候已經剩下不到24小時的時間，突然就發現前三名都還有持續提交模型（？），我的模型預估已經到極限了。</p>\n<p>索性就放棄第一，因為前後訓練的金額達到700美金之多，沒有像他們一樣是有組團那麼多資源，一個人參賽資源還是有限，後來比賽結束之後，大家也有在討論，才發現他們有辦法更改最高速度跟本地訓練，有一些技術可以達到一樣訓練目的，不過這個最高速度在比完這次虛擬賽之後也全面開放到8 m/s，有時候資訊不對稱也會是比賽的弱點之一。也因此決定寫這篇文章，讓大家清楚知道我是如何訓練好一批強勁的Agent，也想讓更多的台灣好手能夠被看見，也許有些人天生有資源，但是這些並不重要，這次本魯能夠靠一個人拿到那麼前面的名次，代表你有實力，很多東西其實沒那麼重要。</p>\n<h2 id=\"實體競賽心得：\">實體競賽心得：<a href=\"#實體競賽心得：\" title=\"實體競賽心得：\"></a></h2><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*BUFvFuvn5Ubrju_ZqGWQ5Q.jpeg\" class=\"φcy\"></p>\n<p>說真的這次去Summit玩看看實體的小車，哇！真的好多東西跟虛擬競賽的不太一樣，原本想說去參觀（我絕對不會說為了贈品），沒想到手有點癢就把之前訓練的幾個實驗模型丟下去跑跑看，結果全部都爆出軌了，</p>\n<p>其實大多跟上週更改每秒判斷次數有關，原本車子判斷次數每秒十次改為15次，這個差別蠻多的，很多訓練有素的模型（含虛擬第四名的模型）在實際賽道上跑起來很悲劇。</p>\n<p>但是這次倒是觀察到很多Agent的行為，認知到在實體賽道上獎勵的設定要以中心線為主，作為整體的策略，如果用白線作為邊界去訓練，很容易因為光線晃動問題而判斷失誤，因此以中心線配合黑色底道路是最佳的選擇。</p>\n<p>也因為這次Summit了解到國內很多人對於這方面很熱心熱血，趨勢科技也自組了一個團隊來玩，交通大學也有一群熱血學生，<strong>真心希望他們可以拿到冠軍出國比賽</strong>，而不是被外國人搶走（這次社群有幾個從國外跑來比的）。</p>\n<h2 id=\"結語：\">結語：<a href=\"#結語：\" title=\"結語：\"></a></h2><p>最後要非常感謝<strong>AWS總經理跟Amanda（正妹推廣者）</strong>以及這次比賽讓我亂試模型的工作人員（我都調100速度不要恨我…），這次去玩還真的受寵若驚，從不知道整個AWS Taiwan的人都有關注我的線上比賽（能不能補助一點呀～啾咪），這個真的有感動到我，以為我是一個人，原來背後有許多人默默支持我，也感謝這次Warren補助我訓練經費，不過真的累了不想訓練太多模型去跟學生們搶獎品，因為第一個虛擬賽道的積分已經拿到，集滿六個其實前18名去拉斯維加斯也沒問題，但是做這系列比賽真的很好玩卻頗累的，還有其他規劃要同時進行，就看後續還有沒有心力繼續比下去了。</p>\n<p>哎呀還要補充一點，因為虛擬前10名的關係有獲得一台小車（還沒拿到），之後應該會有計劃跟AWS借一下場地來讓大家體驗一下實體賽車（沒有實體車的可以用Donkey Car玩玩看）。</p>\n<p>如果本篇文章有幫到你的忙，<br>選單的<a href=\"https://www.paypal.me/ChenKuanSun\" target=\"_blank\">Donate</a>可以請我喝一杯英式紅茶拿鐵，<br>讓我有更多的咖啡因(X)</p>\n","prev":{"title":"LIFF 與 Angular 共舞 — 實作 — 02","link":"2019/06/17/LIFF_02"},"next":{"title":"LIFF 與 Angular 共舞 — 實作 — 01","link":"2019/06/11/LIFF_01"},"plink":"https://chenkuansun.github.io/2019/06/13/AWS_DeepRacer/","toc":[{"id":"實體競賽心得：","title":"實體競賽心得：","index":"1"},{"id":"結語：","title":"結語：","index":"2"}]}