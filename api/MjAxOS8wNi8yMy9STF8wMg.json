{"title":"Unity Obstacle Tower 第一章：介紹","date":"2019-06-22T18:46:25.000Z","date_formatted":{"ll":"Jun 23, 2019","L":"06/23/2019","MM-DD":"06-23"},"link":"2019/06/23/RL_02","comments":true,"tags":["Machine Learning","Reinforcement Learning","Unity ML"],"categories":["Reinforcement Learning"],"updated":"2020-04-13T23:55:01.631Z","content":"<p>Unity今年初推出了一個名為「Obstacle Tower」的遊戲，意圖在讓AI在3D環境中解謎跟逃脫的遊戲。<br>這邊假設大家都對RL有基本的認識，今天帶大家如何切入實戰。</p>\n<h3 id=\"介紹\">介紹<a href=\"#介紹\" title=\"介紹\"></a></h3><p>有接觸過RL的人應該對蒙特祖瑪的復仇這塊不陌生，他是一個解謎遊戲，不只是短期獎勵，還要有長期的策略，但到今天的增強式學習相信大家都有發現一個問題，就是都以2D環境為主，而3D空間只有訓練假人站立等等行為，嚴格來說，假人實際給Agent訓練的State也只是一堆參數而已。今天Unity團隊聯手GCP團隊一起創辦了一個國際挑戰賽，製作出了3D版的蒙特祖瑪的復仇，表面上是挑戰賽，實際上是刺激參賽團隊能夠寫出一個可以處理3D影像的增強式學習模型，所以也有OpenAI的參賽者加入。而作者本身有幸進入決賽，由於最近在設計分層強化學習模型有點卡關，所以藉由寫教學文章順便對基礎重新複習，希望可以在這段期間寫出很優秀的模型。也帶大家正式進入真實RL在玩的領域。</p>\n<h3 id=\"下載並分析環境\">下載並分析環境<a href=\"#下載並分析環境\" title=\"下載並分析環境\"></a></h3><p>1：這邊請先根據您的環境下載環境執行檔案。</p>\n<p><a href=\"https://storage.googleapis.com/obstacle-tower-build/v2.1/obstacletower_v2.1_linux.zip\" target=\"_blank\">Linux (x86_64)</a></p>\n<p><a href=\"https://storage.googleapis.com/obstacle-tower-build/v2.1/obstacletower_v2.1_osx.zip\" target=\"_blank\">Mac OS X</a></p>\n<p><a href=\"https://storage.googleapis.com/obstacle-tower-build/v2.1/obstacletower_v2.1_windows.zip\" target=\"_blank\">Windows</a></p>\n<p>2：解壓縮後打開執行文件</p>\n<p>這邊會開啟遊戲，這邊講解動作空間（Action space)，</p>\n<p>上下左右各是WASD，左右旋轉鏡頭為KL，跳耀為空白鍵</p>\n<p>右上角的Floor是第幾層，Time是剩餘時間，Tower是隨機種子。</p>\n<p>到下一關（下一層）要進入向上箭頭標記的門。</p>\n<p>這裡每一關都是隨機生成的，有100個隨機種子，另外還有不同主題，所以有非常多的樣本。</p>\n<p>可以實際玩一輪看看，人類平均水平在16層，</p>\n<p>這個是綠色的門，只是通道，從第二層開始就要到第二個房間才能上樓</p>\n<p>時間寶珠是用來增加剩餘時間的。</p>\n<p>在第五層開始有鑰匙，有時候鑰匙會在跳台上面。（這對ＡＩ來說是很困難的動作）</p>\n<p>然後第七層左右就會有黑洞，</p>\n<p>第十層就有推箱子，第十層開始會有不同主題，</p>\n<p>要把箱子推到藍色方格上才會開門，如果不小心推到邊邊，踩紅色的方格就會重置方塊。</p>\n<p>然後第11層開始房間會變大一倍，地上還會有這種旁邊有坑的，掉進去就會直接重來。</p>\n<p>後面還有很多莫名其妙的東西，大家可以自己摸索一下，但相信大家也發現困難度在哪了。</p>\n<p>玩的過程要思考如何讓Agent可以順著路走，不會自殺等等，</p>\n<p>執行檔直接給Agent訓練可以讀的的State目前就是168X168X3(可以調成84X84X3的復古模式)的影像，剩餘時間，箱子數，鑰匙數，跟預設獎勵值</p>\n<p>這些就是我們所謂的觀察空間(Observation spaces)，另外獎勵值就是過一個關卡會給你一分，過一個綠門會給0.1等等。</p>\n<p>但我上一句有說，這是預設獎勵值，實際上增強式學習的獎勵函數，都是自己打造，</p>\n<p>而要先瞭解怎麼設計RL就要先了解環境，架構，</p>\n<p>進而找出；</p>\n<pre><code>觀察空間\n動作空間\n潛在的問題</code></pre><p>然後總結這些因素來設計</p>\n<pre><code>輸入的預處理(Feature Engineering)\n輸出的動作組合（Action Table)\n獎勵函數（Reward Function)</code></pre><p>而回饋修正的就是預期的Agent行為跟實際的行為，</p>\n<p>根據不斷人眼觀察實際跟預期的差異，</p>\n<p>去分析是State預處理完的資訊不足還是獎勵函數的設計謬誤，</p>\n<p>這邊我都戲稱為RL心理學，因為很像在調教Agent~</p>\n<p>待續，下一章會講解觀察空間跟動作空間。這段期間大家先玩看看～</p>\n<p>如果本篇文章有幫到你的忙，<br>選單的<a href=\"https://www.paypal.me/ChenKuanSun\" target=\"_blank\">Donate</a>可以請我喝一杯英式紅茶拿鐵，<br>讓我有更多的咖啡因(X)</p>\n","prev":{"title":"使用Visual Studio Code將函數部署到AWS Lambda","link":"2019/07/19/AWS_Serverless_02"},"next":{"title":"使用PyCharm將函數部署到AWS Lambda","link":"2019/06/21/AWS_Serverless_01"},"plink":"https://chenkuansun.github.io/2019/06/23/RL_02/","toc":[{"id":"介紹","title":"介紹","index":"1"},{"id":"下載並分析環境","title":"下載並分析環境","index":"2"}]}